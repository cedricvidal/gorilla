{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.16.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (2.16.1)\n",
      "Requirement already satisfied: openai==1.10.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.10.0)\n",
      "Requirement already satisfied: PyPDF2==3.0.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: transformers==4.37.2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 4)) (4.37.2)\n",
      "Requirement already satisfied: langchain_experimental in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 5)) (0.0.58)\n",
      "Requirement already satisfied: langchain_openai in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 6)) (0.1.5)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: pyyaml==6.0.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 8)) (6.0.1)\n",
      "Requirement already satisfied: coloredlogs==15.0.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: mdc==1.2.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 10)) (1.2.1)\n",
      "Requirement already satisfied: pytest==8.1.2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 11)) (8.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (15.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: pandas in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: packaging in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (0.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: filelock in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from datasets==2.16.1->-r ../requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from openai==1.10.0->-r ../requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from openai==1.10.0->-r ../requirements.txt (line 2)) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from openai==1.10.0->-r ../requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from openai==1.10.0->-r ../requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from openai==1.10.0->-r ../requirements.txt (line 2)) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from openai==1.10.0->-r ../requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from transformers==4.37.2->-r ../requirements.txt (line 4)) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from transformers==4.37.2->-r ../requirements.txt (line 4)) (2024.5.10)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from transformers==4.37.2->-r ../requirements.txt (line 4)) (0.4.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from coloredlogs==15.0.1->-r ../requirements.txt (line 9)) (10.0)\n",
      "Requirement already satisfied: future in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from mdc==1.2.1->-r ../requirements.txt (line 10)) (1.0.0)\n",
      "Requirement already satisfied: python-json-logger in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from mdc==1.2.1->-r ../requirements.txt (line 10)) (2.0.7)\n",
      "Requirement already satisfied: tomli>=1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pytest==8.1.2->-r ../requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.4 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pytest==8.1.2->-r ../requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pytest==8.1.2->-r ../requirements.txt (line 11)) (1.2.1)\n",
      "Requirement already satisfied: iniconfig in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pytest==8.1.2->-r ../requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain_experimental->-r ../requirements.txt (line 5)) (0.1.52)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.17 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain_experimental->-r ../requirements.txt (line 5)) (0.1.20)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain_openai->-r ../requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: idna>=2.8 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.10.0->-r ../requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r ../requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r ../requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r ../requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r ../requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1->-r ../requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: certifi in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.10.0->-r ../requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.10.0->-r ../requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.10.0->-r ../requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (0.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (0.6.6)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (0.1.56)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (8.3.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (2.0.30)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (0.0.38)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain_experimental->-r ../requirements.txt (line 5)) (1.33)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.10.0->-r ../requirements.txt (line 2)) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.10.0->-r ../requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r ../requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.16.1->-r ../requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1->-r ../requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (3.21.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain_experimental->-r ../requirements.txt (line 5)) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (3.10.3)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /workspaces/gorilla/raft/.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental->-r ../requirements.txt (line 5)) (1.0.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data generation phase\n",
    "\n",
    "### Generate Q/A/CoT fine-tuning dataset using RAFT from the domain specific documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset: vampire-bats\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"vampire-bats\"\n",
    "doc_path = \"../sample_data/vampire-bats/\"\n",
    "ds_path = \"dataset/vampire-bats_test\"\n",
    "print(\"Creating dataset: \" + ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[32m2024-05-17 18:18:23\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Using checkpoint chunks /workspaces/gorilla/raft/azure-ai-studio-ft/dataset/vampire-bats_test-checkpoints/chunks\n",
      "\u001b[32m2024-05-17 18:18:23\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats of type pdf\n",
      "\u001b[32m2024-05-17 18:18:24\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/bats/Giant golden-crowned flying fox - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:25\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 51 chunks.\n",
      "\u001b[32m2024-05-17 18:18:27\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/bats/Desmodus draculae - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:30\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 17 chunks.\n",
      "\u001b[32m2024-05-17 18:18:30\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/bats/Vampire bat - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:32\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 68 chunks.\n",
      "\u001b[32m2024-05-17 18:18:34\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/bats/Leaf-nosed bat - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:36\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 61 chunks.\n",
      "\u001b[32m2024-05-17 18:18:38\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/vampires/The Fearless Vampire Killers - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:39\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 29 chunks.\n",
      "\u001b[32m2024-05-17 18:18:40\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/vampires/Abraham Van Helsing - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:42\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 89 chunks.\n",
      "\u001b[32m2024-05-17 18:18:45\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Retrieving chunks from ../sample_data/vampire-bats/vampires/Vampire - Wikipedia.pdf using the text-embedding-ada-002 model.\n",
      "\u001b[32m2024-05-17 18:18:50\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Splitting text into 182 chunks.\n",
      "Saving the dataset (1/1 shards): 100%|█| 498/498 [00:01<00:00, 400.87 examples/s\n",
      "\u001b[32m2024-05-17 18:19:01\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Using system prompt key llama\n",
      "\u001b[32m2024-05-17 18:19:01\u001b[0m \u001b[1;30m INFO\u001b[0m [  0%] \u001b[34mraft\u001b[0m Using checkpoint file /workspaces/gorilla/raft/azure-ai-studio-ft/dataset/vampire-bats_test-checkpoints/checkpoint.txt\n",
      "\u001b[32m2024-05-17 18:19:01\u001b[0m \u001b[1;30m INFO\u001b[0m [  1%] \u001b[34mraft\u001b[0m Adding chunk 1/498\n",
      "\u001b[32m2024-05-17 18:21:32\u001b[0m \u001b[1;30m INFO\u001b[0m [  1%] \u001b[34mraft\u001b[0m Adding chunk 2/498\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! python3 ../raft.py \\\n",
    "    --datapath $doc_path \\\n",
    "    --output $ds_path \\\n",
    "    --distractors 3 \\\n",
    "    --doctype pdf \\\n",
    "    --chunk_size 512 \\\n",
    "    --questions 5 \\\n",
    "    --checkpoint-size 1 \\\n",
    "    --system-prompt-key llama \\\n",
    "    --completion_model Meta-Llama-3-70B-Instruct \\\n",
    "    --embedding_model text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert generated HuggingFace arrow dataset to JSONL format suitable for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading arrow file dataset/vampire-bats/data-00000-of-00001.arrow\n"
     ]
    }
   ],
   "source": [
    "raft_arrow_file = f\"{ds_path}/data-00000-of-00001.arrow\"\n",
    "dataset_path = f\"{ds_path}-files/{ds_name}-full.jsonl\"\n",
    "dataset_path_hf = f\"{ds_path}-files/{ds_name}-hf.full.jsonl\"\n",
    "\n",
    "dataset_path_hf_train = f\"{ds_path}-files/{ds_name}-hf.train.jsonl\"\n",
    "dataset_path_hf_valid = f\"{ds_path}-files/{ds_name}-hf.valid.jsonl\"\n",
    "dataset_path_hf_eval = f\"{ds_path}-files/{ds_name}-hf.eval.jsonl\"\n",
    "\n",
    "dataset_path_ft_train = f\"{ds_path}-files/{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_valid = f\"{ds_path}-files/{ds_name}-ft.valid.jsonl\"\n",
    "dataset_path_ft_eval = f\"{ds_path}-files/{ds_name}-ft.eval.jsonl\"\n",
    "\n",
    "\n",
    "print(f\"Reading arrow file {raft_arrow_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 1461 examples [00:00, 14295.26 examples/s]\n",
      "\u001b[32m2024-05-16 04:50:35\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Converting arrow file dataset/vampire-bats/data-00000-of-00001.arrow to jsonl hf file dataset/vampire-bats-files/vampire-bats-hf.full.jsonl\n",
      "Creating json from Arrow format: 100%|████████████| 2/2 [00:00<00:00,  5.54ba/s]\n"
     ]
    }
   ],
   "source": [
    "! python ../format.py \\\n",
    "    --input $raft_arrow_file \\\n",
    "    --output $dataset_path_hf \\\n",
    "    --output-format hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seed_task_0</td>\n",
       "      <td>general</td>\n",
       "      <td>When did Darren Naish publish \"What did giant extinct vampire bats eat?\"?</td>\n",
       "      <td>{'sentences': [['(2003). \"Late quaternary bats from Cebada Cave,\n",
       "Chiquibul cave system, Belize\". Caribbean Journal of Science . 39 (1): 23–33. 5. Pardiñas, U. F . J.; Tonni, E. P .', 'Czaplewski, N. J.; Krejca, J.; Miller , T. E.', '(2000). \"A  giant vampire (Mammalia, Chiroptera) in the Late\n",
       "Holocene from the Argentinean pampas: paleoenvironmental signiﬁcance\". Palaeogeography ,\n",
       "Palaeoclimatology , Palaeoecology .', '(2005). \"Taxonomic status of the cuban vampire bat (Chiroptera: Phyllostomidae:\n",
       "Desmodontinae: Desmodus)\"  (http://www .redciencia.cu/geobiblio/paper/2005_Suarez,%20W ._Tax\n",
       "onomic%20Status%20of%20the%20Cuban%20V ampire%20Bat.pdf)  (PDF) . Caribbean Journal of\n",
       "Science .']], 'title': [['placeholder_title', 'placeholder_title', 'placeholder_title', 'placeholder_title']]}</td>\n",
       "      <td>101 (4):\\n912–928. 7. Naish, Darren  (July 14, 2013). \"What did giant extinct vampire bats eat?\"  (https://blogs.scientiﬁca\\nmerican.com/tetrapod-zoology/what-did-giant-extinct-vampire-bats-eat/) . ScientiﬁcAmerican.com .</td>\n",
       "      <td>To answer the question, we need to identify when Darren Naish published \"What did giant extinct vampire bats eat?\".\\n\\nThe context provided gives us the necessary information to determine this.\\n\\nFirst, we look for the part of the context that directly mentions Darren Naish's publication. \\nWe find it in the sentence: ##begin_quote##7. Naish, Darren  (July 14, 2013). \"What did giant extinct vampire bats eat?\"  (https://blogs.scientificamerican.com/tetrapod-zoology/what-did-giant-extinct-vampire-bats-eat/) . ScientiﬁcAmerican.com .##end_quote##.\\n\\nFrom this sentence, we understand that the publication \"What did giant extinct vampire bats eat?\" was written by Darren Naish, and the date of publication is explicitly mentioned as July 14, 2013.\\n\\nTherefore, based on the context provided, we can conclude that Darren Naish published \"What did giant extinct vampire bats eat?\" on July 14, 2013.\\n\\n&lt;ANSWER&gt;: July 14, 2013</td>\n",
       "      <td>&lt;DOCUMENT&gt;(2003). \"Late quaternary bats from Cebada Cave,\\nChiquibul cave system, Belize\". Caribbean Journal of Science . 39 (1): 23–33. 5. Pardiñas, U. F . J.; Tonni, E. P .&lt;/DOCUMENT&gt;\\n&lt;DOCUMENT&gt;Czaplewski, N. J.; Krejca, J.; Miller , T. E.&lt;/DOCUMENT&gt;\\n&lt;DOCUMENT&gt;(2000). \"A  giant vampire (Mammalia, Chiroptera) in the Late\\nHolocene from the Argentinean pampas: paleoenvironmental signiﬁcance\". Palaeogeography ,\\nPalaeoclimatology , Palaeoecology .&lt;/DOCUMENT&gt;\\n&lt;DOCUMENT&gt;(2005). \"Taxonomic status of the cuban vampire bat (Chiroptera: Phyllostomidae:\\nDesmodontinae: Desmodus)\"  (http://www .redciencia.cu/geobiblio/paper/2005_Suarez,%20W ._Tax\\nonomic%20Status%20of%20the%20Cuban%20V ampire%20Bat.pdf)  (PDF) . Caribbean Journal of\\nScience .&lt;/DOCUMENT&gt;\\nWhen did Darren Naish publish \"What did giant extinct vampire bats eat?\"?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     type  \\\n",
       "0  seed_task_0  general   \n",
       "\n",
       "                                                                    question  \\\n",
       "0  When did Darren Naish publish \"What did giant extinct vampire bats eat?\"?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    context  \\\n",
       "0  {'sentences': [['(2003). \"Late quaternary bats from Cebada Cave,\n",
       "Chiquibul cave system, Belize\". Caribbean Journal of Science . 39 (1): 23–33. 5. Pardiñas, U. F . J.; Tonni, E. P .', 'Czaplewski, N. J.; Krejca, J.; Miller , T. E.', '(2000). \"A  giant vampire (Mammalia, Chiroptera) in the Late\n",
       "Holocene from the Argentinean pampas: paleoenvironmental signiﬁcance\". Palaeogeography ,\n",
       "Palaeoclimatology , Palaeoecology .', '(2005). \"Taxonomic status of the cuban vampire bat (Chiroptera: Phyllostomidae:\n",
       "Desmodontinae: Desmodus)\"  (http://www .redciencia.cu/geobiblio/paper/2005_Suarez,%20W ._Tax\n",
       "onomic%20Status%20of%20the%20Cuban%20V ampire%20Bat.pdf)  (PDF) . Caribbean Journal of\n",
       "Science .']], 'title': [['placeholder_title', 'placeholder_title', 'placeholder_title', 'placeholder_title']]}   \n",
       "\n",
       "                                                                                                                                                                                                                  oracle_context  \\\n",
       "0  101 (4):\\n912–928. 7. Naish, Darren  (July 14, 2013). \"What did giant extinct vampire bats eat?\"  (https://blogs.scientiﬁca\\nmerican.com/tetrapod-zoology/what-did-giant-extinct-vampire-bats-eat/) . ScientiﬁcAmerican.com .   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         cot_answer  \\\n",
       "0  To answer the question, we need to identify when Darren Naish published \"What did giant extinct vampire bats eat?\".\\n\\nThe context provided gives us the necessary information to determine this.\\n\\nFirst, we look for the part of the context that directly mentions Darren Naish's publication. \\nWe find it in the sentence: ##begin_quote##7. Naish, Darren  (July 14, 2013). \"What did giant extinct vampire bats eat?\"  (https://blogs.scientificamerican.com/tetrapod-zoology/what-did-giant-extinct-vampire-bats-eat/) . ScientiﬁcAmerican.com .##end_quote##.\\n\\nFrom this sentence, we understand that the publication \"What did giant extinct vampire bats eat?\" was written by Darren Naish, and the date of publication is explicitly mentioned as July 14, 2013.\\n\\nTherefore, based on the context provided, we can conclude that Darren Naish published \"What did giant extinct vampire bats eat?\" on July 14, 2013.\\n\\n<ANSWER>: July 14, 2013   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         instruction  \n",
       "0  <DOCUMENT>(2003). \"Late quaternary bats from Cebada Cave,\\nChiquibul cave system, Belize\". Caribbean Journal of Science . 39 (1): 23–33. 5. Pardiñas, U. F . J.; Tonni, E. P .</DOCUMENT>\\n<DOCUMENT>Czaplewski, N. J.; Krejca, J.; Miller , T. E.</DOCUMENT>\\n<DOCUMENT>(2000). \"A  giant vampire (Mammalia, Chiroptera) in the Late\\nHolocene from the Argentinean pampas: paleoenvironmental signiﬁcance\". Palaeogeography ,\\nPalaeoclimatology , Palaeoecology .</DOCUMENT>\\n<DOCUMENT>(2005). \"Taxonomic status of the cuban vampire bat (Chiroptera: Phyllostomidae:\\nDesmodontinae: Desmodus)\"  (http://www .redciencia.cu/geobiblio/paper/2005_Suarez,%20W ._Tax\\nonomic%20Status%20of%20the%20Cuban%20V ampire%20Bat.pdf)  (PDF) . Caribbean Journal of\\nScience .</DOCUMENT>\\nWhen did Darren Naish publish \"What did giant extinct vampire bats eat?\"?  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "hf_full_df = pd.read_json(dataset_path_hf, lines=True)\n",
    "hf_full_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/gorilla/raft/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 80%/20%\n",
    "import numpy as np\n",
    "samples_count = len(hf_full_df)\n",
    "hf_train_df, hf_valid_df, hf_eval_df = np.split(hf_full_df, [int(.8*samples_count), int(.9*samples_count)])\n",
    "hf_train_df.to_json(dataset_path_hf_train, orient=\"records\", lines=True)\n",
    "hf_valid_df.to_json(dataset_path_hf_valid, orient=\"records\", lines=True)\n",
    "hf_eval_df.to_json(dataset_path_hf_eval, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 1168 examples [00:00, 13747.96 examples/s]\n",
      "\u001b[32m2024-05-16 04:57:04\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Converting jsonl file dataset/vampire-bats-files/vampire-bats-hf.train.jsonl to jsonl completion file dataset/vampire-bats-files/vampire-bats-ft.train.jsonl\n",
      "Map: 100%|████████████████████████| 1168/1168 [00:00<00:00, 13310.48 examples/s]\n",
      "Creating json from Arrow format: 100%|████████████| 2/2 [00:00<00:00,  8.74ba/s]\n"
     ]
    }
   ],
   "source": [
    "! python ../format.py \\\n",
    "    --input $dataset_path_hf_train \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_train \\\n",
    "    --output-format completion \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train split: 146 examples [00:00, 9187.13 examples/s]\n",
      "\u001b[32m2024-05-16 04:57:15\u001b[0m \u001b[1;30m INFO\u001b[0m [    ] \u001b[34mraft\u001b[0m Converting jsonl file dataset/vampire-bats-files/vampire-bats-hf.valid.jsonl to jsonl completion file dataset/vampire-bats-files/vampire-bats-ft.valid.jsonl\n",
      "Map: 100%|██████████████████████████| 146/146 [00:00<00:00, 20351.90 examples/s]\n",
      "Creating json from Arrow format: 100%|████████████| 1/1 [00:00<00:00, 82.84ba/s]\n"
     ]
    }
   ],
   "source": [
    "! python ../format.py \\\n",
    "    --input $dataset_path_hf_valid \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_valid \\\n",
    "    --output-format completion \\\n",
    "    --output-completion-prompt-column text\\\n",
    "    --output-completion-completion-column ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning phase\n",
    "\n",
    "### Loading the model to fine-tune\n",
    "\n",
    "We will use the `llama-2-7b` model to show how user can finetune a model for text-completion task. If you opened this notebook from a specific model card, remember to replace the specific model name. Optionally, if you need to fine tune a model that is available on HuggingFace, but not available in `azureml` system registry, to do so [import](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/system/import/import_model_into_registry.ipynb) the model.\n",
    "\n",
    "### Outline\n",
    "* Pick a model to fine-tune.\n",
    "* Pick and explore training data.\n",
    "* Configure the fine tuning job.\n",
    "* Run the fine tuning job.\n",
    "* Review training metrics.\n",
    "* Deploy the fine tuned model for real time inference. [TODO]\n",
    "* Clean up resources.  [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup pre-requisites\n",
    "* Install dependencies\n",
    "* Connect to AzureML Workspace. Learn more at [set up SDK authentication](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk). Replace  `<WORKSPACE_NAME>`, `<RESOURCE_GROUP>` and `<SUBSCRIPTION_ID>` below.\n",
    "* Connect to `azureml` system registry\n",
    "* Set an optional experiment name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies by running below cell. This is not an optional step if running in a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-storage-file-datalake==12.14.0\n",
    "%pip install azure-ai-ml\n",
    "%pip install azure-identity\n",
    "\n",
    "%pip install mlflow\n",
    "%pip install azureml-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for download hugging face datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!az login --use-device-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "    print(\"Loaded ML Client configuration from config.json\")\n",
    "except:\n",
    "    print(\"Loading ML Client configuration directly\")\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )\n",
    "\n",
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml\"\n",
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "registry_ml_client_meta = MLClient(credential, registry_name=\"azureml-meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a foundation model to fine tune\n",
    "\n",
    "Decoder based LLM models like `llama` performs well on `text-completion` tasks, we need to finetune the model for our specific purpose in order to use it. You can browse these models in the Model Catalog in the AzureML Studio, filtering by the `text-completion` task. In this example, we use the `llama-2-7b` model. If you have opened this notebook for a different model, replace the model name and version accordingly. \n",
    "\n",
    "Note the model id property of the model. This will be passed as input to the fine tuning job. This is also available as the `Asset ID` field in model details page in AzureML Studio Model Catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(registry_ml_client_meta.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-2-7b\"\n",
    "#model_name = \"Meta-Llama-3-8B\"\n",
    "foundation_model = registry_ml_client_meta.models.get(model_name, label=\"latest\")\n",
    "print(f\"Using model name: {foundation_model.name}, version: {foundation_model.version}, id: {foundation_model.id} for fine tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants._common import AssetTypes\n",
    "from azure.ai.ml.entities._inputs_outputs import Input\n",
    "mlflow_model_llama = Input(type=AssetTypes.MLFLOW_MODEL, path=foundation_model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pick the dataset for fine-tuning the model\n",
    "\n",
    "We use the [samsum](https://huggingface.co/datasets/samsum) dataset. The next few cells show basic data preparation for fine tuning:\n",
    "* Visualize some data rows\n",
    "* Preprocess the data and format it in required format. This is an important step for performing text completion as we add the required sequences/separators in the data. This is how we repurpose the text-completion task to any specific task like summarization, translation, text-completion, etc.\n",
    "* While fintuning, text column is concatenated with ground_truth column to produce finetuning input. Hence, the data should be prepared such that `text + ground_truth` is your actual finetuning data.\n",
    "* bos and eos tokens are added to the data by finetuning pipeline, you do not need to add it explicitly \n",
    "* We want this sample to run quickly, so save smaller `train`, `validation` and `test` files containing 10% of the original. This means the fine tuned model will have lower accuracy, hence it should not be put to real-world use. \n",
    "\n",
    "##### Here is an example of how the data should look like\n",
    "\n",
    "text completion requires the training data to include at least 2 fields – one for ‘text’ and ‘ground_truth’ like in this example. The below examples are from Samsum dataset. \n",
    "\n",
    "Original dataset:\n",
    "\n",
    "| dialogue (text) | summary (ground_truth) |\n",
    "| :- | :- |\n",
    "| Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :) | Eric and Rob are going to watch a stand-up on youtube. | \n",
    "| Will: hey babe, what do you want for dinner tonight?\\r\\nEmma:  gah, don't even worry about it tonight\\r\\nWill: what do you mean? everything ok?\\r\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\r\\nWill: Well what time will you be home?\\r\\nEmma: soon, hopefully\\r\\nWill: you sure? Maybe you want me to pick you up?\\r\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \\r\\nWill: Alright, love you. \\r\\nEmma: love you too. | Emma will be home soon and she will let Will know. | \n",
    "\n",
    "Formatted dataset the user might pass:\n",
    "\n",
    "| text (text) | summary (ground_truth) |\n",
    "| :- | :- |\n",
    "| Summarize this dialog:\\nEric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :)\\n---\\nSummary:\\n | Eric and Rob are going to watch a stand-up on youtube. | \n",
    "| Summarize this dialog:\\nWill: hey babe, what do you want for dinner tonight?\\r\\nEmma:  gah, don't even worry about it tonight\\r\\nWill: what do you mean? everything ok?\\r\\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\\r\\nWill: Well what time will you be home?\\r\\nEmma: soon, hopefully\\r\\nWill: you sure? Maybe you want me to pick you up?\\r\\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \\r\\nWill: Alright, love you. \\r\\nEmma: love you too. \\n---\\nSummary:\\n | Emma will be home soon and she will let Will know. | \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ./samsum-dataset/train.jsonl file into a pandas dataframe and show the first 5 rows\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", 0\n",
    ")  # set the max column width to 0 to display the full text\n",
    "train_df = pd.read_json(dataset_path_ft_train, lines=True)\n",
    "valid_df = pd.read_json(dataset_path_ft_valid, lines=True)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Supported paths include:\n",
    "# local: './<path>/<file>'\n",
    "# blob:  'https://<account_name>.blob.core.windows.net/<container_name>/<path>/<file>'\n",
    "# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'\n",
    "# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'\n",
    "#my_path = './dataset/intents-pc-16k-test-1999.jsonl'\n",
    "\n",
    "#my_data = Data(path=my_path, type=AssetTypes.URI_FILE, name=\"intents-pc-16k-test-1999\")\n",
    "\n",
    "#workspace_ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Submit the fine tuning job using the the model and data as inputs\n",
    " \n",
    "Create the job that uses the `text-generation` pipeline component. [Learn more](https://github.com/Azure/azureml-assets/blob/main/assets/training/finetune_acft_hf_nlp/components/pipeline_components/text_generation/README.md) about all the parameters supported for fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define finetune parameters\n",
    "\n",
    "Finetune parameters can be grouped into 2 categories - training parameters, optimization parameters\n",
    "\n",
    "Training parameters define the training aspects such as - \n",
    "1. the optimizer, scheduler to use\n",
    "2. the metric to optimize the finetune\n",
    "3. number of training steps and the batch size\n",
    "and so on\n",
    "\n",
    "Optimization parameters help in optimizing the GPU memory and effectively using the compute resources. Below are few of the parameters that belong to this category. _The optimization parameters differs for each model and are packaged with the model to handle these variations._\n",
    "1. enable the deepspeed, ORT and LoRA\n",
    "2. enable mixed precision training\n",
    "2. enable multi-node training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities._inputs_outputs import Input\n",
    "training_data=Input(type=\"uri_file\", path=dataset_path_ft_train)\n",
    "validation_data=Input(type=\"uri_file\", path=dataset_path_ft_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create FineTuning job object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "guid = uuid.uuid4()\n",
    "short_guid = str(guid)[:8]\n",
    "experiment_name = f\"raft-{ds_name}\"\n",
    "registered_model_name = f\"{experiment_name}-{short_guid}\"\n",
    "print(\"experiment_name = \" + experiment_name)\n",
    "print(\"registered_model_name = \" + registered_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities._job.finetuning.custom_model_finetuning_job import CustomModelFineTuningJob\n",
    "from azure.ai.ml._restclient.v2024_01_01_preview.models import FineTuningTaskType\n",
    "from azure.ai.ml.entities._inputs_outputs import Output\n",
    "\n",
    "custom_model_finetuning_job = CustomModelFineTuningJob(\n",
    "    task=FineTuningTaskType.TEXT_COMPLETION,\n",
    "    training_data=training_data,\n",
    "    validation_data=validation_data,\n",
    "    hyperparameters={\n",
    "        \"per_device_train_batch_size\": \"1\",\n",
    "        \"learning_rate\": \"0.0002\",\n",
    "        \"num_train_epochs\": \"1\",\n",
    "    },\n",
    "    model=mlflow_model_llama,\n",
    "    display_name=registered_model_name,\n",
    "    name=registered_model_name,\n",
    "    experiment_name=experiment_name,\n",
    "    tags={\"agent\": \"gorilla-raft-notebook\"},\n",
    "    properties={},\n",
    "    outputs={\"registered_model\": Output(type=\"mlflow_model\", name=registered_model_name)},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit FineTuningJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_job = workspace_ml_client.jobs.create_or_update(custom_model_finetuning_job)\n",
    "created_job.studio_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Review training and evaluation metrics\n",
    "Viewing the job in AzureML studio is the best way to analyze logs, metrics and outputs of jobs. You can create custom charts and compare metics across different jobs. See https://learn.microsoft.com/en-us/azure/machine-learning/how-to-log-view-metrics?tabs=interactive#view-jobsruns-information-in-the-studio to learn more. \n",
    "\n",
    "However, we may need to access and review metrics programmatically for which we will use MLflow, which is the recommended client for logging and querying metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_override = \"llama-762084ae\"\n",
    "if job_id_override:\n",
    "    job_id = job_id_override\n",
    "else:\n",
    "    job_id = created_job.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow, json\n",
    "\n",
    "mlflow_tracking_uri = workspace_ml_client.workspaces.get(\n",
    "    workspace_ml_client.workspace_name\n",
    ").mlflow_tracking_uri\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "# concat 'tags.mlflow.rootRunId=' and pipeline_job.name in single quotes as filter variable\n",
    "filter = \"tags.mlflow.rootRunId='\" + job_id + \"'\"\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[experiment_name], filter_string=filter, output_format=\"list\"\n",
    ")\n",
    "training_run = None\n",
    "evaluation_run = None\n",
    "# get the training and evaluation runs.\n",
    "# using a hacky way till 'Bug 2320997: not able to show eval metrics in FT notebooks - mlflow client now showing display names' is fixed\n",
    "for run in runs:\n",
    "    # check if run.data.metrics.epoch exists\n",
    "    if \"epoch\" in run.data.metrics:\n",
    "        training_run = run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_run:\n",
    "    print(\"Training metrics:\\n\\n\")\n",
    "    print(json.dumps(training_run.data.metrics, indent=2))\n",
    "else:\n",
    "    print(\"No Training job found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(workspace_ml_client.models.list())\n",
    "registered_model = models[-1]\n",
    "registered_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Serverless deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Marketplace Sub Name, Serverless Endpoint Name, and Model ID\n",
    "\n",
    "**Note**: Make sure your `serverless_endpoint_name` is unique!\n",
    "\n",
    "You can use any of these model ids for your endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serverless_model_ids = [\n",
    "    \"azureml://registries/azureml-mistral/models/Mistral-large\",\n",
    "    \"azureml://registries/azureml-meta/models/Meta-Llama-3-8B-Instruct\",\n",
    "    \"azureml://registries/azureml-meta/models/Meta-Llama-3-70B-Instruct\",\n",
    "    \"azureml://registries/azureml-cohere/models/Cohere-embed-v3-multilingual\",\n",
    "    \"azureml://registries/azureml-cohere/models/Cohere-embed-v3-english\",\n",
    "    \"azureml://registries/azureml-cohere/models/Cohere-command-r\",\n",
    "    \"azureml://registries/azureml-cohere/models/Cohere-command-r-plus\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model._to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in registry_ml_client_meta.models.list():\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"Llama-2-7b\"\n",
    "model_name = \"Meta-Llama-3-8B-Instruct\"\n",
    "deployment_model = registry_ml_client_meta.models.get(model_name, label=\"latest\")\n",
    "print(f\"Using model name: {deployment_model.name}, version: {deployment_model.version}, id: {deployment_model.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_model._to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"azureml://locations/westus3/workspaces/24827e2c-b602-428c-943b-e9c0204b82cf/models/default-registered-model-name/versions/1\"\n",
    "#model_id = \"azureml://registries/azureml-meta/models/Llama-2-7b\"\n",
    "model_id = \"azureml://registries/azureml-meta/models/Meta-Llama-3-8B\"\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marketplace_sub_info(model):\n",
    "    return (f\"mrkt-sub-{model.name}\", f\"{model.name}-endpoint\"[:32], model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_model = registered_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-ml==1.16.0a20240501006 --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subscribe_model_id(model_id):\n",
    "  model_name = model_id.split(\"/\")[-1]\n",
    "  marketplace_sub_name = f\"mrkt-sub-{model_name}\"\n",
    "  marketplace_subscription = MarketplaceSubscription(\n",
    "    name=marketplace_sub_name,\n",
    "    model_id=model_id\n",
    "  )\n",
    "\n",
    "  marketplace_subscription = workspace_ml_client.marketplace_subscriptions.begin_create_or_update(marketplace_subscription).result()\n",
    "  return marketplace_subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import MarketplaceSubscription, ServerlessEndpoint\n",
    "\n",
    "model_id = \"azureml://registries/azureml-meta/models/Meta-Llama-3-8B-Instruct\"\n",
    "marketplace_subscription = subscribe_model_id(model_id)\n",
    "print(marketplace_subscription.as_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deploying model id {model_id} to serverless endpoint {serverless_endpoint_name}\")\n",
    "\n",
    "serverless_endpoint = ServerlessEndpoint(\n",
    "  name=serverless_endpoint_name,\n",
    "  model_id=model_id\n",
    ")\n",
    "\n",
    "created_endpoint = workspace_ml_client.serverless_endpoints.begin_create_or_update(serverless_endpoint).result()\n",
    "\n",
    "print(created_endpoint.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Deploy the fine tuned model to an online endpoint [TODO: Need some work]\n",
    "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "online_endpoint_name = \"samsum-textgen-\" + timestamp\n",
    "online_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    OnlineRequestSettings,\n",
    ")\n",
    "\n",
    "# Create online endpoint - endpoint names need to be unique in a region, hence using timestamp to create unique endpoint name\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Online endpoint for \"\n",
    "    + registered_model.name\n",
    "    + \", fine tuned model for samsum textgen\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "workspace_ml_client.begin_create_or_update(endpoint).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find here the list of SKU's supported for deployment - [Managed online endpoints SKU list](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployment\n",
    "demo_deployment = ManagedOnlineDeployment(\n",
    "    name=\"demo\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model.id,\n",
    "    instance_type=\"Standard_E64s_v3\",\n",
    "    instance_count=1,\n",
    "    liveness_probe=ProbeSettings(initial_delay=600),\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=90000),\n",
    ")\n",
    "workspace_ml_client.online_deployments.begin_create_or_update(demo_deployment).wait()\n",
    "endpoint.traffic = {\"demo\": 100}\n",
    "workspace_ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Test the endpoint with sample data\n",
    "\n",
    "We will fetch some sample data from the test dataset and submit to online endpoint for inference. We will then show the display the scored labels alongside the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ./samsum-dataset/small_test.jsonl into a pandas dataframe\n",
    "test_df = pd.read_json(\"./samsum-dataset/small_test.jsonl\", lines=True)\n",
    "# take 5 random samples\n",
    "test_df = test_df.sample(n=2)\n",
    "# rebuild index\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "# rename the label_string column to ground_truth_label\n",
    "test_df = test_df.rename(columns={\"label_string\": \"ground_truth_label\"})\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a json object with the key as \"input_data\" and value as a list of values from the text column of the test dataframe\n",
    "test_json = {\"input_data\": {\"text\": list(test_df[\"text\"])}}\n",
    "# save the json object to a file named sample_score.json in the ./samsum-dataset folder\n",
    "with open(\"./samsum-dataset/sample_score.json\", \"w\") as f:\n",
    "    json.dump(test_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the sample_score.json file using the online endpoint with the azureml endpoint invoke method\n",
    "response = workspace_ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"demo\",\n",
    "    request_file=\"./samsum-dataset/sample_score.json\",\n",
    ")\n",
    "print(\"raw response: \\n\", response, \"\\n\")\n",
    "# convert the response to a pandas dataframe and rename the label column as scored_label\n",
    "response_df = pd.read_json(response)\n",
    "response_df = response_df.rename(columns={0: \"scored_label\"})\n",
    "response_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the test dataframe and the response dataframe on the index\n",
    "merged_df = pd.merge(test_df, response_df, left_index=True, right_index=True)\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Delete the online endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_endpoints.begin_delete(name=online_endpoint_name).wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
