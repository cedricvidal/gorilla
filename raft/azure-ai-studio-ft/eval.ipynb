{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation with Data\n",
    "In this notebook, we introduce built-in evaluators and guide you through creating your own custom evaluators. We'll cover both code-based and prompt-based custom evaluators. Finally, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e99e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing any old installation\n",
    "# This is important since older version of promptflow has one package.\n",
    "# Now it is split into number of them.\n",
    "! pip uninstall -y promptflow promptflow-cli promptflow-azure promptflow-core promptflow-devkit promptflow-tools promptflow-evals\n",
    "\n",
    "# Install packages in this order\n",
    "! pip install promptflow-evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75440bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install azure_ai_ml --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n",
    "\n",
    "# Dependencies needed for some of the notebooks\n",
    "#! pip install azure-cli\n",
    "#! pip install bs4\n",
    "#! pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a40006",
   "metadata": {},
   "source": [
    "Expected env vars\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_API_KEY\n",
    "AZURE_OPENAI_API_VERSION\n",
    "AZURE_OPENAI_DEPLOYMENT\n",
    "AZURE_OPENAI_ENDPOINT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da43d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee20226",
   "metadata": {},
   "source": [
    "## 0. Prepare eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b110e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_hf_eval = \"dataset/hf.eval.jsonl\"\n",
    "dataset_path_ft_eval = \"dataset/ft.eval.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../format.py \\\n",
    "    --input $dataset_path_hf_eval \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_eval \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "## 1. Built-in Evaluators\n",
    "\n",
    "The table below lists all the built-in evaluators we support. In the following sections, we will select a few of these evaluators to demonstrate how to use them.\n",
    "\n",
    "| Category       | Namespace                                        | Evaluator Class           | Notes                                             |\n",
    "|----------------|--------------------------------------------------|---------------------------|---------------------------------------------------|\n",
    "| Quality        | promptflow.evals.evaluators                      | GroundednessEvaluator     |                                                   |\n",
    "|                |                                                  | RelevanceEvaluator        |                                                   |\n",
    "|                |                                                  | CoherenceEvaluator        |                                                   |\n",
    "|                |                                                  | FluencyEvaluator          |                                                   |\n",
    "|                |                                                  | SimilarityEvaluator       |                                                   |\n",
    "|                |                                                  | F1ScoreEvaluator          |                                                   |\n",
    "| Content Safety | promptflow.evals.evaluators.content_safety       | ViolenceEvaluator         |                                                   |\n",
    "|                |                                                  | SexualEvaluator           |                                                   |\n",
    "|                |                                                  | SelfHarmEvaluator         |                                                   |\n",
    "|                |                                                  | HateUnfairnessEvaluator   |                                                   |\n",
    "| Composite      | promptflow.evals.evaluators                      | QAEvaluator               | Built on top of individual quality evaluators.    |\n",
    "|                |                                                  | ChatEvaluator             | Similar to QAEvaluator but designed for evaluating chat messages. |\n",
    "|                |                                                  | ContentSafetyEvaluator    | Built on top of individual content safety evaluators. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "### 1.1 Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_version=os.environ.get(\"OPENAI_API_VERSION\")\n",
    "\n",
    "print(\"azure_endpoint=\" + azure_endpoint)\n",
    "print(\"azure_deployment=\" + azure_deployment)\n",
    "print(\"api_version=\" + api_version)\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf075450-1835-4b15-8002-76ae249caab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67844e07-57bb-415f-b1af-606b5a67aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## 3. Using Evaluate API to evaluate with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2265e",
   "metadata": {},
   "source": [
    "First, let's take a peek at what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"data.jsonl\"\n",
    "\n",
    "df = pd.read_json(data_path, lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using a few evaluators that we already initialized\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"ground_truth\": \"${data.truth}\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dadc4b-495f-4a46-ad15-d4c8a910b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, JSON\n",
    "\n",
    "display(JSON(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb28f9a-1f9c-4c2e-8b32-e7da51585f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "print(result[\"studio_url\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
